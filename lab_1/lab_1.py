# -*- coding: utf-8 -*-
"""lab_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19buT4bVEtAHDTKAW5wM-9RE886TZtGIg
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from scipy.spatial.distance import pdist
from scipy.cluster.hierarchy import *
import pandas as pd
from sklearn.datasets import load_digits
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest,chi2,RFE
from sklearn.ensemble import RandomForestClassifier
import plotly.figure_factory as ff
# %matplotlib inline

method = False
nClust=6

from google.colab import drive
drive.mount('/content/drive')
df = pd.read_csv('drive/MyDrive/ColabNotebooks/binary.dat', sep=';')
df

label = df["A"]
df.drop("A", axis=1, inplace=True)
print('\n\nЗначение метки "A":')
print(label.value_counts())
label.value_counts().plot(kind="bar")



print(df.info())

clf = RandomForestClassifier()
clf.fit(df, label)
# create a figure to plot a bar, where x axis is features, and Y indicating the importance of each feature
plt.figure(figsize=(12,12))
plt.bar(df.columns, clf.feature_importances_)
plt.xticks(rotation=45)

fig = ff.create_dendrogram(df)
fig.update_layout(width=800, height=500)
fig.show()

#Распределили объекты по кластерам
data_dist = pdist(df, 'euclidean')
data_linkage = linkage(data_dist, method='average')
clusters=fcluster(data_linkage, nClust, criterion='maxclust')
clusters

# к оригинальным данным добавляем номер кластера
col=["B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L",
       "M", "N", "O", "P", "Q"]
df['Cluster']=clusters
res=df.groupby('Cluster')[col].mean()
res['Количество']=df.groupby('Cluster').size().values
res #ниже средние цифры по кластерам и количество объектов (Количество)

#Import required module
from sklearn.cluster import KMeans
data = pd.read_csv('drive/MyDrive/ColabNotebooks/binary.dat', sep=';')
data.drop("A", axis=1, inplace=True)
pca = PCA(2)
df = pca.fit_transform(data)
df.shape
#Initialize the class object
kmeans = KMeans(n_clusters= 6)
 
#predict the labels of clusters.
label = kmeans.fit_predict(df)
 
print(label)

#Getting the Centroids
centroids = kmeans.cluster_centers_
u_labels = np.unique(label)
 
#plotting the results:
 
for i in u_labels:
    plt.scatter(df[label == i , 0] , df[label == i , 1] , label = i)
plt.scatter(centroids[:,0] , centroids[:,1] , s = 80, color = 'k')
plt.legend()
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
 
df_features = pd.read_csv('drive/MyDrive/ColabNotebooks/binary.dat', sep=';') 
df_features.drop("A", axis=1, inplace=True)
 
SSE = [] # хранить сумму квадратов ошибок для каждого результата
for k in range(1,10):
    estimator = KMeans (n_clusters = k) # построить кластер
    estimator.fit(df_features[['B', "C", "D", "E", "F", "G", "H", "I", "J", "K", "L",
       "M", "N", "O", "P", "Q"]])
    SSE.append(estimator.inertia_)
X = range(1,10)
plt.xlabel('k')
plt.ylabel('SSE')
plt.plot(X,SSE,'o-')

plt.show()

